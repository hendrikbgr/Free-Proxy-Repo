<div align="center">
  <img src="gif.gif" alt="alt text" width="2000px">
  <h1>FREE FRESH DAILY PROXY LIST</h1>
  <h3>UPDATED EVERY HOUR</h3>
  <a href="https://raw.githubusercontent.com/hendrikbgr/Free-Proxy-Repo/master/proxy_list.txt" target="_blank">-> Download Now <-</a>
</div>

<hr>

<div align="center">
  <h3>Proxy-Pool-API | Powered by this Repo</h3>
  <br>
  <a href="https://proxy-pool-api.onrender.com" target="_blank">
    <img src="proxy.gif" alt="alt text" width="2000px">
  </a>
  <p>Access a 100% free proxy pool API! Quickly and easily integrate proxies into your projects.</p>
  <br>
  <p> Visit <a href="https://proxy-pool-api.onrender.com" target="_blank">proxy-pool-api.onrender.com</a> to get started.</p>
</div>

<hr>

# ğŸ”¥ Version 0.0.3 ğŸ”¥

This is the first version of my fully automated GitHub repo & proxy scraper. My proxy scraper & checker runs on my local Raspberry Pi 4+ and updates the proxy list once scraped and checked. After that, the whole process restarts.

My script crawls over 60+ websites and more than 270+ URLs to find all public proxies available. All proxies are checked before being updated here.

# Proxy Scraper & Checker

You can find the Proxy Scraper's repo here: [Proxy-Scraper](https://github.com/hendrikbgr/Proxy-Scraper)

ğŸš€ Automate your Proxy Scraping ğŸš€

ğŸ“Œ Ver. 0.0.3 ğŸ“Œ

## Features

- Scrape all public proxies from preset URLs.
- Scrape all links from the preset URLs.
  - Scrape all public proxies from the discovered links.
- Save all scraped proxies to a file.
- Remove duplicate proxies.
- Check all proxies and save them to a file.
  - Checked on www.google.com.

### Support ğŸ‘¨â€ğŸ’»

#### Contact ğŸ“©

If you encounter any issues with running the script or have questions, feel free to reach out to me:

- **Twitter:** [@hendrik_bgr](https://twitter.com/Hendrik_bgr)
- **Email:** [hendriksdevmail@gmail.com](mailto:hendriksdevmail@gmail.com)
